# Feature Specification: Unified Agentic RAG Chatbot System

**Feature Branch**: `010-agentic-rag-chatbot`
**Created**: 2025-12-20
**Status**: Draft
**Input**: User description: "Unified Agentic RAG Chatbot System that can answer questions about books using RAG, questions based on selected text only, and general questions. Implements a router endpoint POST /agent/query that classifies queries into three modes: selected text mode, book RAG mode, and general knowledge mode. Must reuse existing RAG pipeline without modifications."

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Ask questions about book content using RAG (Priority: P1)

As a user, I want to ask questions about book content and receive accurate answers with sources, so that I can understand the book material better.

**Why this priority**: This is the core functionality that leverages the existing RAG pipeline to provide value to users asking book-related questions.

**Independent Test**: Can be fully tested by sending a question about book content to the `/agent/query` endpoint and verifying that the response contains an answer with sources and the mode is set to "book".

**Acceptance Scenarios**:

1. **Given** I have a book in the system with searchable content, **When** I ask a question about the book content, **Then** I receive a relevant answer with cited sources from the book.
2. **Given** I submit a question to the `/agent/query` endpoint, **When** the system determines it's related to book content, **Then** the response includes the answer, sources array populated with document references, and mode set to "book".

---

### User Story 2 - Ask questions based on selected text only (Priority: P1)

As a user, I want to ask questions about specific text I've selected from a book and receive answers only from that text, so that I can get focused explanations without interference from other book content.

**Why this priority**: This provides a critical functionality for users who want to understand specific passages without the system pulling information from elsewhere in the book.

**Independent Test**: Can be fully tested by sending a question with selected_text parameter to the `/agent/query` endpoint and verifying that the response is generated only from the provided text.

**Acceptance Scenarios**:

1. **Given** I have selected specific text from a book, **When** I ask a question with the selected_text parameter, **Then** the response is generated only from the provided text without using external RAG sources.
2. **Given** I submit a query with selected_text to the `/agent/query` endpoint, **When** the system identifies selected_text is present, **Then** the response contains an answer based solely on the provided text and mode is set to "selected_text".

---

### User Story 3 - Ask general questions not related to the book (Priority: P2)

As a user, I want to ask general questions that are not related to the book and receive answers from the LLM, so that I can get responses to general knowledge questions without leaving the interface.

**Why this priority**: This enhances the user experience by providing a complete question-answering solution that doesn't require switching contexts for general questions.

**Independent Test**: Can be fully tested by sending a general knowledge question to the `/agent/query` endpoint and verifying that the response comes from the LLM without using RAG sources.

**Acceptance Scenarios**:

1. **Given** I ask a general knowledge question not related to the book, **When** I submit it to the `/agent/query` endpoint, **Then** I receive an answer generated by the LLM without any book sources.
2. **Given** I submit a query without selected_text and unrelated to book content, **When** the system determines it's a general knowledge question, **Then** the response contains an answer with empty sources array and mode set to "general".

---

### Edge Cases

- What happens when the selected_text is extremely long and exceeds token limits?
- How does the system handle queries that could be interpreted as both book-related and general knowledge?
- What happens when the RAG system is temporarily unavailable but the user asks a book-related question?
- How does the system handle ambiguous queries that could fit multiple categories?
- What occurs when the selected_text parameter is provided but is empty or null?

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: System MUST provide a new API endpoint POST `/agent/query` that acts as a central decision-making agent for routing queries.
- **FR-002**: System MUST classify incoming queries into one of three modes: selected_text, book, or general based on query content and parameters.
- **FR-003**: System MUST route queries to appropriate answering strategies based on the determined mode.
- **FR-004**: When selected_text parameter is provided, system MUST generate answers using only the provided text without accessing external RAG sources.
- **FR-005**: When query is related to book content, system MUST reuse the existing `/query` RAG pipeline internally to retrieve relevant information.
- **FR-006**: When query is general knowledge not related to the book, system MUST use direct LLM completion without retrieval.
- **FR-007**: System MUST maintain the existing `/query` endpoint unchanged and not modify its functionality.
- **FR-008**: System MUST not modify existing database schemas or data structures.
- **FR-009**: System MUST not re-run existing ingestion or embedding logic when processing queries.
- **FR-010**: All responses from `/agent/query` endpoint MUST follow the unified response format with answer, sources, and mode fields.
- **FR-011**: System MUST return responses in the format: `{ "answer": "string", "sources": [], "mode": "selected_text | book | general" }`.
- **FR-012**: System MUST act as a wrapper layer without duplicating existing vector search or embedding logic.

### Key Entities

- **Query**: A user's question with optional parameters including selected_text, representing the input to the system.
- **Agent Router**: The central decision-making component that classifies and routes queries to appropriate answering strategies.
- **Query Mode**: The classification of a query as one of three types: selected_text, book, or general.
- **Answer**: The response generated by the appropriate answering strategy based on the query mode.
- **Sources**: A list of document references or citations that support the answer, primarily used for book-related queries.
- **Answering Strategy**: The method used to generate the answer, which varies based on the query mode (selected-text-only, RAG-based, or general LLM completion).

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: Users can receive accurate answers to book-related questions through the `/agent/query` endpoint with 90% accuracy compared to direct RAG queries.
- **SC-002**: Selected-text queries are answered with 95% adherence to only the provided text without hallucinating information from other sources.
- **SC-003**: General knowledge questions receive relevant answers within 5 seconds response time.
- **SC-004**: The system correctly classifies and routes 95% of incoming queries to the appropriate answering strategy.
- **SC-005**: The existing `/query` endpoint continues to function without any performance degradation or breaking changes.
- **SC-006**: The unified response format is consistently returned for all query types with proper mode identification.